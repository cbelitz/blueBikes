---
title: "Final Report: Boston Bluebikes"
author: "Clara Belitz and Phil Graff"
date: "5/12/2021"
output: pdf_document
fig_caption: yes
---

```{r echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, include=FALSE}

# set global plot size options so they look better; also: echo = FALSE! now default! R won't show on PDF unless we set to true
knitr::opts_chunk$set(fig.width=8, fig.height=4, echo = FALSE, warning=FALSE, message=FALSE)

# Call libraries

library(tidyverse)  # I think we want tidyverse? May take out if not needed, or if conflicts with other functions

# Maybe for the other functions, we can just call them where they are needed.

library(knitr)

```

# Background / Problem Space

Bike shares have helped cities meet the transportation needs of their citizens in a way that promotes healthy activity and reduces negative environmental impact. Boston launched their Bluebikes program in 2011 with 60 stations. It has grown each year and now hosts 380 stations across the Boston metro area.

In this report, we will analyze bike usage through the lens of possible stakeholders: local officials and urban planners in participating municipalities and bike share operators. We will attempt to predict ride starts and flow (ride ends minus ride starts) on a daily basis at each participating station in the month of September 2019.


# Data Sources and Processing

## Data Sources

Boston Bluebikes publishes their anonymized and cleaned bike rental data online. The cleaned data has removed trips that are taken by staff as they service and inspect the system; and any trips that were below 60 seconds in length. Individual ride data was grouped and summarized by station and day.

In addition to the ride data, we are including the following external data sources:

* Weather: Cyclists are exposed to the elements, so weather conditions may have an impact on a personâ€™s decision to rent a bike on a given day. From Weather Underground we can obtain the historical record of daily weather data for Boston, Our analysis uses average temperature and precipitation.

* Distance to T stop: Because bike shares are part of city-wide infrastructure, we may expect popular stations to be located at or near public transit stops.

* Distance to bike lane: The proximity to bike lanes may influence the popularity of stations. We can calculate distance to the nearest bike lane from a given station.

## Data Processing

During exploratory data analysis, we plotted total rides by day of week (Figure \ref{fig:figweekdayend} below) and discovered two things. First, there is a pattern of higher total rides on weekdays versus weekends. We can recode the day of week predictor to a weekend/weekday factor to reduce the dimensionality. Second, Monday Sept 2 appears to be an outlier due to the federal holiday Labor Day.

```{r figweekdayend, fig.cap="\\label{fig:figweekdayend}Rides by Date"}

tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment
# Categorize weekday/weekend
trips$weekdayend <- as.factor(ifelse(trips$weekday %in% c("Sunday","Saturday"), "weekend", "weekday"))
attach(trips)

daily <- trips %>% group_by(date, weekday, weekdayend, Precipitation, TempAvg) %>% summarize(starts = sum(total.start), ends = sum(total.end))

# PLOT: Daily Starts (Rides) by Date
#plot(daily$date,daily$starts)
ggplot(data = daily, aes(date, starts, colour = weekdayend)) +
  geom_point(size = 3) +
  geom_abline(intercept = 0, slope = 0, size = 1) +
  ggtitle("Total Rides by Date") +
  xlab("Date") +
  ylab("Total Rides")

detach(trips)
rm(daily)
rm(trips)

```


# Research Questions

**RQ1:** Can we classify different stations based on their cluster patterns?

**RQ2:** Can we predict bike rental usage by station, and by extension the positive/negative flow of bikes at that station?

**RQ3:** Can we interpret our results?

# RQ1: Classifying Stations

We wanted to classify the stations into three bins based on their daily average rental starts. We used K-means clustering algorithm to assign each station an activity level classification of high, medium, or low. These values aid in interpreting plots, but they can also be used in the later methods as a way to reduce the high dimensionality of one-hot encoded name variables, keeping in mind that these classifications are derived from ride starts, therefore circular in predicting ride starts.

In Figure \ref{fig:figkmeans} below, the stratified bands of color demonstrate how the K-means algorithm grouped these observations according to their activity level.

```{r}
# ^^ 100 words

# READ in the trips by station, day data
tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment

### K-MEANS CLUSTER

# It seems like it might be helpful, rather than having a factor with 328 levels (each station), maybe to have a grouping
# of activity level, so we could have low-medium-high or some such thing? Would we be able to use clustering models for this?
# This would define an attribute of the station

# Critical question: Is it inappropriate to use k-means on single-vector data? Most of the examples seem to use two-dimensional

set.seed(1)
#total_daily_rides_per_stn <- trips %>% group_by(id) %>% summarise(Trips = sum(total.start)) # not needed
avg_daily_rides_per_stn <- trips %>% group_by(id) %>% summarise(Trips = mean(total.start))
km.out = kmeans(avg_daily_rides_per_stn$Trips, 3 , nstart = 20)
```


```{r figkmeans, fig.cap="\\label{fig:figkmeans}K Means"}
ggplot(data = avg_daily_rides_per_stn, aes(id, Trips, colour = factor(km.out$cluster))) +
  scale_color_discrete(labels = c('low','medium','high')) +
  labs(color = "Activity Class") +
  geom_point(size = 2) +
  ggtitle("K-Means Clustering to Determine Activity Level")
```


```{r}
# APPEND THE ACTIVITY LEVEL TO TRIPS

temp <- data.frame(avg_daily_rides_per_stn$id,as.factor(km.out$cluster))
names(temp) <- c("id","activity_class")

levels(temp$activity_class) <- c("low", "medium", "high")

# DANGER: APPENDING TO SELF; DO NOT RUN MULTIPLE TIMES
#trips <- merge(trips, temp)
trips <- merge(trips, temp)
rm(temp) # cleanup

```

# RQ2: Predicting Usage and Flow

## Linear Model

```{r}
# READ in the trips by station, day data
# tripClasses is going to force-read the data into R data types that I want them to be
tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment

### RECLASSIFY DAY OF WEEK AS WEEKDAY or WEEKEND
# Categorize weekday/weekend
trips$weekdayend <- as.factor(ifelse(trips$weekday %in% c("Sunday","Saturday"), "weekend", "weekday"))

```

```{r}
############################
### MODELING, PREDICTION ###
############################
library(leaps)

# REMOVE NAs from TRIPS
clean_trips <- na.omit(trips)

# SELECT TRAINING DATA
set.seed(517)
# Create a vector for training, sampling 75% (or .75) of the dataset
train.v <- sample(nrow(clean_trips), nrow(clean_trips) * .75, replace = FALSE)

#train.noholiday <- train.v[clean_trips[train.v,]$date != '2019-09-02']
#train.noholiday <- clean_trips[train.v,] %>% filter(date != '2019-09-02') %>% select(id) %>% unlist %>% as.numeric()
#test.noholiday <- clean_trips[-train.v,] %>% filter(date != '2019-09-02') %>% select(id) %>% unlist %>% as.numeric()


### LM STARTS ###
# CREATE a COPY of the clean_trips dataset WITH ONLY THE PREDICTORS WE WANT TO CONSIDER
ctrips.predictors <- clean_trips[,c(2:3,5:8,10:12,14,19)]

# TRAIN STARTS
lm_starts_withName <- lm(total.start ~ ., data = ctrips.predictors[train.v,])
lm_starts_withoutName <- lm(total.start ~ ., data = ctrips.predictors[train.v,-5]) # -5 in the data columns removes Name.
#summary(lm_starts) # R^2 with names: 0.89. without names: 0.24
#plot(lm1)


# TRAIN FLOW
lm_flow_withName <- lm(flow ~ ., data = ctrips.predictors[train.v,])
lm_flow_withoutName <- lm(flow ~ ., data = ctrips.predictors[train.v,-5]) # -5 in the data columns removes Name.
#summary(lm_flow) # R^2 with names: 0.37. without names: 0.03



### PREDICTING STARTS ###
pred.starts_withName <- predict(lm_starts_withName, ctrips.predictors[-train.v,])
pred.starts_withName.MSE <- mean((pred.starts_withName - ctrips.predictors$total.start[-train.v])^2)
#pred.starts_withName.MSE
# MSE with names: 237.6147
pred.starts_withoutName <- predict(lm_starts_withoutName, ctrips.predictors[-train.v,])
pred.starts_withoutName.MSE <- mean((pred.starts_withoutName - ctrips.predictors$total.start[-train.v])^2)
#pred.starts_withoutName.MSE
# MSE without names: 1460.618

### PREDICTING FLOW ###
pred.flow_withName <- predict(lm_flow_withName, ctrips.predictors[-train.v,])
pred.flow_withName.MSE <- mean((pred.flow_withName - ctrips.predictors$total.start[-train.v])^2)
#pred.flow_withName.MSE
# MSE with names: 3682.01
pred.flow_withoutName <- predict(lm_flow_withoutName, ctrips.predictors[-train.v,])
# MSE without names: 3590.09
```

To successfully build a linear model, first we want to explore the independent variables and select the most effective. Because the station names are a factor, best subset selection was too computationally intensive so we proceeded with forward stepwise analysis.

```{r echo=FALSE, warning=FALSE, error = FALSE}
### SUBSET SELECTION
# Best selection set too large
regfit.full <- regsubsets(total.start ~ ., data = ctrips.predictors[train.v,], nvmax = 344, method = "forward") #SUBSET ROWS BY TRAIN
full.sum.starts <- summary(regfit.full)
#names(full.sum.starts)
```


```{r figlmselect, fig.height=6, fig.cap="\\label{fig:figlmselect}Choosing the best model"}
# Plot the adjustments to help choose the best predictors
par(mfrow=c(2,2))

# Plot RSS
plot(full.sum.starts$rss, xlab = "Number of variables", ylab = "RSS", type = "l")
starts.min.rss <- which.min(full.sum.starts$rss) # Duh, this will always be the max number of parameters!!
points(starts.min.rss,full.sum.starts$rss[starts.min.rss], col = "red", cex = 2, pch = 20)

# Plot Adjusted R^2
plot(full.sum.starts$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
starts.max.adjr <- which.max(full.sum.starts$adjr2)
points(starts.max.adjr,full.sum.starts$adjr2[starts.max.adjr], col = "red", cex = 2, pch = 20)

# Plot Cp
plot(full.sum.starts$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
starts.min.cp <- which.min(full.sum.starts$cp)
points(starts.min.cp,full.sum.starts$cp[starts.min.cp], col = "red", cex = 2, pch = 20)

# Plot BIC
plot(full.sum.starts$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
starts.min.bic <- which.min(full.sum.starts$bic)
points(starts.min.bic,full.sum.starts$bic[starts.min.bic], col = "red", cex = 2, pch = 20)
```

Although The Bayesian Information Criterion (BIC), which more heavily penalizes additional terms, suggested we could use a smaller model, other selection criteria suggested we include all of the variables in the model for increased accuracy. With station names included, the top five most influential variables were: ``r regfit.full$xnames[regfit.full$vorder][2:6]``


When using linear regression to predict total starts and station flow, we find that the accuracy of the model depends heavily on whether we include Names or not. Predicting starts with station Name has an R^2 of ``r summary(lm_starts_withName)$r.squared`` and a mean square error of``r pred.starts_withName.MSE``. Without station Name the R^2 increases to ``r summary(lm_starts_withoutName)$r.squared`` and the mean square error to ``r pred.starts_withoutName.MSE``. 

The linear model did not predict flow well, with an R^2 of ``r summary(lm_flow_withName)$r.squared`` and a mean square error of ``r pred.flow_withName.MSE``. Without names, the R^2 was a dismal ``r summary(lm_flow_withoutName)$r.squared``.

```{r figlmpredstarts, fig.cap="\\label{fig:lmpredstarts}Predicting Starts with Linear Model, including Station Names"}
ggplot(data = ctrips.predictors[-train.v,], aes(total.start, pred.starts_withName)) +
  geom_point(size = 2) +
  geom_abline(slope =1, intercept = 0, color = "red") +
  ggtitle("Predicting Total Starts (with Names)") +
  xlab("Actual Starts") +
  ylab("Predicted Starts")
```


```{r}
# Linear Modeling environment cleanup
rm(list=ls())
```


## Tree Models

We began by performing cross validation for both starts and flows in order to see what the best number of trees and predictors would be for random forests. As we can see in the graphs below, for both flow and starts, $m=\sqrt{p}$, where m is number of predictors used and p is total possible predictors, performs the worst, while higher numbers of m perform better. 

For flows, $m=p/2=4$ performs very similarly to $m=p=9$, so we will use 4 since it will run faster. It appears to continue improving up to 100 trees. For starts, $m=p=8$ is clearly superior, so we use that with 60 trees, where it stabilizes.

```{r figCVTrees, echo=FALSE, warning=FALSE, message=FALSE, out.width="45%"}
# no figure caption is included here because I want the images on the same line and can't figure out how to get both of captions and one line
include_graphics(c("images/cv_tree_factor_flow.png","images/cv_tree_factor_starts.png"))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(randomForest)
library(gbm)

# READ in the trips by station, day data
# tripClasses is going to force-read the data into R data types that I want them to be
tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment

# Categorize weekday/weekend
trips$weekdayend <- as.factor(ifelse(trips$weekday %in% c("Sunday","Saturday"), "weekend", "weekday"))

# remove missing values
clean_trips = na.omit(trips)

# get training sets
set.seed(1)
train = sample(1:nrow(clean_trips), nrow(clean_trips)*0.75)
trips.test=clean_trips[-train ,"flow"]
starts.test = clean_trips[-train ,"total.start"]

# best flow
set.seed(1)
rf.trips.best = randomForest(flow~weekdayend+District+Total.docks+bikelanedist+tstopdist+total.start+TempAvg+WindAvg+Precipitation,
                        data=clean_trips,
                        subset=train,
                        mtry=4,
                        ntree=100,
                        importance=TRUE)
set.seed(1)
yhat.rf.best = predict(rf.trips.best, newdata=clean_trips[-train,])
sprintf("Mean squared error on the test data for flow is %f",mean((yhat.rf.best-trips.test)^2))
plot(trips.test-yhat.rf.best, ylab="Residuals", main="Residuals for RF Trips")

# best starts
set.seed(1)
rf.starts.best = randomForest(total.start~weekdayend+District+Total.docks+bikelanedist+tstopdist+TempAvg+WindAvg+Precipitation,
                         data=clean_trips,
                         subset=train,
                         mtry=8,
                         ntree=60)
yhat.starts = predict(rf.starts.best, newdata=clean_trips[-train,])
sprintf("Mean squared error on the test data for flow is %f", mean((yhat.starts-starts.test)^2)) 
rm(list=ls())
```

The best results for flow so far are superior to those found in the linear model, with MSE of 47 for flow and 158 for starts. We now want to see if we can get superior results by building a model for each activity level. We can also compare the most important predictors for the models created at each level to see if it varies by activity level. 

Cross-Validation was also performed for these models, but the charts are omitted here. Instead, we will present only the results for the best models for each activity class.

MSE for the high flow data is now much higher (768), while medium is similar but larger (62), and low (21) flow is much smaller. This makes sense given that "low" is the largest activity class, therefore lowering the test error overall when all classes are considered together. Overall, breaking modeling each activity class individually does not dramatically affect which predictors are most important (Figures \ref{fig:figImportancePlotsHigh}, \ref{fig:figImportancePlotsMedium}, \ref{fig:figImportancePlotsLow}). Though the exact order may change, the total number of docks and starts (measures of use) and T-stop and bike lane distances (measures of remoteness) are the most important factors. The District and whether it is a weekend jump around a bit in importance, while weather data has the smallest impact for all activity classes. The results were similar for starts when broken out by activity class.

```{r figImportancePlotsHigh, out.width="80%", fig.cap="\\label{fig:figImportancePlotsHigh}Importance Plot for High Flow"}
include_graphics(c("images/flow_high_importance.png"))
```

```{r figImportancePlotsMedium, out.width="80%", fig.cap="\\label{fig:figImportancePlotsMedium}Importance Plot for Medium Flow"}
include_graphics(c("images/flow_medium_importance.png"))
```

```{r figImportancePlotsLow, out.width="80%", fig.cap="\\label{fig:figImportancePlotsLow}Importance Plot for Low Flow"}
include_graphics(c("images/flow_low_importance.png"))
```

## Support Vector Machines

After tuning, the best models we found are presented below. We use ony 25% of the data for training in order to allow the model to run in a reasonable amount of time. The best models for flow had a radial kernel with gamma 10 and cost 0.5. The best model for starts had a radial kernel 
```{r}
library(e1071)

# READ in the trips by station, day data
# tripClasses is going to force-read the data into R data types that I want them to be
tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment

# Categorize weekday/weekend
trips$weekdayend <- as.factor(ifelse(trips$weekday %in% c("Sunday","Saturday"), "weekend", "weekday"))

# remove missing values
clean_trips = na.omit(trips)

# get training set
set.seed(1)
train = sample(1:nrow(clean_trips), nrow(clean_trips)*.25)
trips.test=clean_trips[-train ,"flow"]
starts.test = clean_trips[-train ,"total.start"]

# these values provided the best value after cross-validation for flow
svr.trips.best = svm(flow~weekdayend+Name+District+Total.docks+bikelanedist+tstopdist+total.start+TempAvg+WindAvg+Precipitation,
                data=clean_trips,
                subset=train,
                kernel="radial",
                gamma=10,
                cost=0.5)
svr.trips.best
set.seed(1)
svr.predict = predict(svr.trips.best, newdata=clean_trips[-train,])
sprintf("Mean squared error on the test data for flow is %f", mean((svr.predict-trips.test)^2))

# starts with best values from cv
set.seed(1)
svr.starts.best = svm(total.start~weekdayend+Name+District+Total.docks+bikelanedist+tstopdist+TempAvg+WindAvg+Precipitation,
                data=clean_trips,
                subset=train,
                kernel="radial",
                gamma=0.1,
                cost=20)
svr.starts.best
set.seed(1)
svr.starts.predict = predict(svr.starts.best, newdata=clean_trips[-train,])
sprintf("Mean squared error on the test data for flow is %f", mean((svr.starts.predict-starts.test)^2))
plot(svr.starts.predict-starts.test, ylab="Residuals", main="Residuals for SVR Starts")
rm(list=ls())
```

Because SVM did not do as well as random forests, we chose not to build SVM models for each of the activity classes. It did, however, still perform better than linear methods. We can see that the errors for starts are well pretty well centered around 0, which is another positive feature of this model. 

## Local Regression

Because linear regression tended not to perform well at the extremes, we thought local regression might be a good approach. But, because the most important factor tended to be name, which is categorical and highly multi-dimensional, it was not a good candidate for things like local regression. None of the models we tried for local regression were superior to other non-linear approaches like trees or SVM.

# RQ3: Interpreting Results

Because the most successful models were decision trees, our results are easily interpreted. Decision trees allow us to see the relative importance of measures of remoteness (T-stop and Bike Lane distance), as well as measures of unsurprising factors, like those that correlate with use (Docks). An added benefit of trees is that they did not use the name of the stations as a predictor, allowing them to be utilized for predicting bike usage at a newly built station. This type of prediction is likely to be of interest to city planners and bike operators.


# Discussion and Conclusion
Because trees performed well on this data, we are able to see how important infrastructure is for predicting bike usage. This data could be used, for example, to argue that increased bike infrastructure leads to increased bike usage, and that bikes are a vital part of a city's general public transit systems. 

We also saw that weather does not vary enough within a month to have a strong effect on the data. We would be curious to see the effect that weather has from month to month, since we imagine that a snowstorm, for example, may decrease ridership.

If we had more time, we would also bring in additional outside data that could help us understand the usage at a more granular level. This could include T usage data, as well as neighborhood information like average income or demographics.

Full project source code available at: https://github.com/cbelitz/blueBikes

# Final note about collaboration

-once the rest of the document is complete, a space for each of us to reflect on these questions:
How did your team work together? Did team members work together on pieces of the
project, or split the project into different topics for different team members?
