---
title: "Final Report: Boston Bluebikes"
author: "Clara Belitz and Phil Graff"
date: "5/12/2021"
output: pdf_document
fig_caption: yes
---

```{r echo = FALSE, warning=FALSE, message=FALSE}

# set global plot size options so they look better; also: echo = FALSE! now default! R won't show on PDF unless we set to true
knitr::opts_chunk$set(fig.width=8, fig.height=4, echo = FALSE, warning=FALSE, message=FALSE)

# Call libraries

library(tidyverse)  # I think we want tidyverse? May take out if not needed, or if conflicts with other functions

# Maybe for the other functions, we can just call them where they are needed.

library(knitr)

```

# Background / Problem Space

Bike shares have helped cities meet the transportation needs of their citizens in a way that promotes healthy activity and reduces negative environmental impact. Boston launched their Bluebikes program in 2011 with 60 stations. It has grown each year and now hosts 380 stations across the Boston metro area.

In this report, we will analyze bike usage through the lens of possible stakeholders: local officials and urban planners in participating municipalities and bike share operators. We will attempt to predict ride starts and flow (ride ends minus ride starts) on a daily basis at each participating station in the month of September 2019.


# Data Sources and Processing

## Data Sources

Boston Bluebikes publishes their anonymized and cleaned bike rental data online. The cleaned data has removed trips that are taken by staff as they service and inspect the system; and any trips that were below 60 seconds in length. Individual ride data was grouped and summarized by station and day.

In addition to the ride data, we are including the following external data sources:

* Weather: Cyclists are exposed to the elements, so weather conditions may have an impact on a personâ€™s decision to rent a bike on a given day. From Weather Underground we can obtain the historical record of daily weather data for Boston, Our analysis uses average temperature and precipitation.

* Distance to T stop: Because bike shares are part of city-wide infrastructure, we may expect popular stations to be located at or near public transit stops.

* Distance to bike lane: The proximity to bike lanes may influence the popularity of stations. We can calculate distance to the nearest bike lane from a given station.

## Data Processing

During exploratory data analysis, we plotted total rides by day of week (Figure \ref{fig:figweekdayend} below) and discovered two things. First, there is a pattern of higher total rides on weekdays versus weekends. We can recode the day of week predictor to a weekend/weekday factor to reduce the dimensionality. Second, Monday Sept 2 appears to be an outlier due to the federal holiday Labor Day.

```{r figweekdayend, fig.cap="\\label{fig:figweekdayend}Rides by Date"}

tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment
# Categorize weekday/weekend
trips$weekdayend <- as.factor(ifelse(trips$weekday %in% c("Sunday","Saturday"), "weekend", "weekday"))
attach(trips)

daily <- trips %>% group_by(date, weekday, weekdayend, Precipitation, TempAvg) %>% summarize(starts = sum(total.start), ends = sum(total.end))

# PLOT: Daily Starts (Rides) by Date
#plot(daily$date,daily$starts)
ggplot(data = daily, aes(date, starts, colour = weekdayend)) +
  geom_point(size = 3) +
  geom_abline(intercept = 0, slope = 0, size = 1) +
  ggtitle("Total Rides by Date") +
  xlab("Date") +
  ylab("Total Rides")

detach(trips)
rm(daily)
rm(trips)

```


# Research Questions

**RQ1:** Can we classify different stations based on their cluster patterns?

**RQ2:** Can we predict bike rental usage by station, and by extension the positive/negative flow of bikes at that station?

**RQ3:** Can we interpret our results?

# RQ1: Classifying Stations

We wanted to classify the stations into three bins based on their daily average rental starts. We used K-means clustering algorithm to assign each station an activity level classification of high, medium, or low. These values aid in interpreting plots, but they can also be used in the later methods as a way to reduce the high dimensionality of one-hot encoded name variables, keeping in mind that these classifications are derived from ride starts, therefore circular in predicting ride starts.

In Figure \ref{fig:figkmeans} below, the stratified bands of color demonstrate how the K-means algorithm grouped these observations according to their activity level.

```{r}
# ^^ 100 words

# READ in the trips by station, day data
tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment

### K-MEANS CLUSTER

# It seems like it might be helpful, rather than having a factor with 328 levels (each station), maybe to have a grouping
# of activity level, so we could have low-medium-high or some such thing? Would we be able to use clustering models for this?
# This would define an attribute of the station

# Critical question: Is it inappropriate to use k-means on single-vector data? Most of the examples seem to use two-dimensional

set.seed(1)
#total_daily_rides_per_stn <- trips %>% group_by(id) %>% summarise(Trips = sum(total.start)) # not needed
avg_daily_rides_per_stn <- trips %>% group_by(id) %>% summarise(Trips = mean(total.start))
km.out = kmeans(avg_daily_rides_per_stn$Trips, 3 , nstart = 20)
```


```{r figkmeans, fig.cap="\\label{fig:figkmeans}K Means"}
ggplot(data = avg_daily_rides_per_stn, aes(id, Trips, colour = factor(km.out$cluster))) +
  scale_color_discrete(labels = c('low','medium','high')) +
  labs(color = "Activity Class") +
  geom_point(size = 2) +
  ggtitle("K-Means Clustering to Determine Activity Level")
```


```{r}
# APPEND THE ACTIVITY LEVEL TO TRIPS

temp <- data.frame(avg_daily_rides_per_stn$id,as.factor(km.out$cluster))
names(temp) <- c("id","activity_class")

levels(temp$activity_class) <- c("low", "medium", "high")

# DANGER: APPENDING TO SELF; DO NOT RUN MULTIPLE TIMES
#trips <- merge(trips, temp)
trips <- merge(trips, temp)
rm(temp) # cleanup

```

# RQ2: Predicting Usage and Flow

## Linear Model

PHIL -- 

```{r}
# READ in the trips by station, day data
# tripClasses is going to force-read the data into R data types that I want them to be
tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment

### RECLASSIFY DAY OF WEEK AS WEEKDAY or WEEKEND
# Categorize weekday/weekend
trips$weekdayend <- as.factor(ifelse(trips$weekday %in% c("Sunday","Saturday"), "weekend", "weekday"))

```

```{r}
############################
### MODELING, PREDICTION ###
############################
library(leaps)

# REMOVE NAs from TRIPS
clean_trips <- na.omit(trips)

# SELECT TRAINING DATA
set.seed(517)
# Create a vector for training, sampling 75% (or .75) of the dataset
train.v <- sample(nrow(clean_trips), nrow(clean_trips) * .75, replace = FALSE)

### LM STARTS ###
# CREATE a COPY of the clean_trips dataset WITH ONLY THE PREDICTORS WE WANT TO CONSIDER for STARTS
ctrips.predictors.start <- clean_trips[,c(2:3,5:8,10:12,14,19)]

# TRAIN STARTS
lm_starts <- lm(total.start ~ ., data = ctrips.predictors.start[train.v,-5]) # -5 in the data columns removes Name.
summary(lm_starts) # R^2 with names: 0.89. without names: 0.24
#plot(lm1)


### LM FLOW ###
# CREATE a COPY of the clean_trips dataset WITH ONLY THE PREDICTORS WE WANT TO CONSIDER for FLOW
# ... actually I think they are the same -- the response variable is pulled out and we are left with the rest
ctrips.predictors.flow <- clean_trips[,c(2:3,5:8,10:12,14,19)]

# TRAIN FLOW
lm_flow <- lm(flow ~ ., data = ctrips.predictors.flow[train.v,]) # -5 in the data columns removes Name.
summary(lm_flow) # R^2 with names: 0.37. without names: 0.03



### PREDICTING STARTS ###
pred.starts <- predict(lm_starts, ctrips.predictors.start[-train.v,])
pred.starts.MSE <- mean((pred.starts - ctrips.predictors.start$total.start[-train.v])^2)
pred.starts.MSE
# MSE with names: 237.6147
# MSE without names: 1460.618

### PREDICTING FLOW ###
pred.flow <- predict(lm_flow, ctrips.predictors.start[-train.v,])
pred.flow.MSE <- mean((pred.flow - ctrips.predictors.start$total.start[-train.v])^2)
pred.flow.MSE
# MSE with names: 3682.01
# MSE without names: 3590.09
```


```{r figlmpredstarts, fig.cap="\\label{fig:figkmeans}Predicting "}
ggplot(data = ctrips.predictors.start[-train.v,], aes(total.start, pred.starts)) +
  geom_point(size = 2) +
  geom_abline(slope = coef(lm_starts)[[2]], intercept = coef(lm_starts)[[1]], color = "red") +
  ggtitle("Predicting Total Starts (without Names)") +
  xlab("Actual Starts") +
  ylab("Predicted Starts")
```


```{r}
### SUBSET SELECTION
# Best selection set too large
regfit.full <- regsubsets(total.start ~ ., data = ctrips.predictors.start[train.v,], nvmax = 344, method = "forward") #SUBSET ROWS BY TRAIN
full.sum.starts <- summary(regfit.full)
names(full.sum.starts)
```


```{r figlmselect, fig.cap="\\label{fig:figlmselect}K Means"}
# Plot the adjustments to help choose the best predictors
par(mfrow=c(2,2))

# Plot RSS
plot(full.sum.starts$rss, xlab = "Number of variables", ylab = "RSS", type = "l")
starts.min.rss <- which.min(full.sum.starts$rss) # Duh, this will always be the max number of parameters!!
points(starts.min.rss,full.sum.starts$rss[starts.min.rss], col = "red", cex = 2, pch = 20)

# Plot Adjusted R^2
plot(full.sum.starts$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
starts.max.adjr <- which.max(full.sum.starts$adjr2)
points(starts.max.adjr,full.sum.starts$adjr2[starts.max.adjr], col = "red", cex = 2, pch = 20)

# Plot Cp
plot(full.sum.starts$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
starts.min.cp <- which.min(full.sum.starts$cp)
points(starts.min.cp,full.sum.starts$cp[starts.min.cp], col = "red", cex = 2, pch = 20)

# Plot BIC
plot(full.sum.starts$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
starts.min.bic <- which.min(full.sum.starts$bic)
points(starts.min.bic,full.sum.starts$bic[starts.min.bic], col = "red", cex = 2, pch = 20)

head(regfit.full$xnames[regfit.full$vorder])
```
```{r}
# Linear Modeling environment cleanup
rm(list=ls())
```


## Tree Models

We began by performing cross validation for both starts and flows in order to see what the best number of trees and predictors would be for random forests. As we can see in the graphs below, for both flow and starts, $m=\sqrt{p}$, where m is number of predictors used and p is total possible predictors, performs the worst, while higher numbers of m perform better. 

For flows, $m=p/2=4$ performs very similarly to $m=p=9$, so we will use 4 since it will run faster. It appears to continue improving up to 100 trees. For starts, $m=p=8$ is clearly superior, so we use that with 60 trees, where it stabilizes.

```{r figCVTrees, echo=FALSE, warning=FALSE, message=FALSE, out.width="45%"}
# no figure caption is included here because I want the images on the same line and can't figure out how to get both of captions and one line
include_graphics(c("images/cv_tree_factor_flow.png","images/cv_tree_factor_starts.png"))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(randomForest)
library(gbm)

# READ in the trips by station, day data
# tripClasses is going to force-read the data into R data types that I want them to be
tripClasses <- c("factor", "Date", "integer", "integer", "integer", "factor", "factor", "factor", "factor", "integer", "numeric", "numeric", "integer", "numeric", "integer", "integer", "numeric","integer","numeric")
trips <- read.csv("data/trips_per_day_09_19.csv", colClasses = tripClasses)
rm(tripClasses) #cleanup environment

# Categorize weekday/weekend
trips$weekdayend <- as.factor(ifelse(trips$weekday %in% c("Sunday","Saturday"), "weekend", "weekday"))

# remove missing values
clean_trips = na.omit(trips)

# get training sets
set.seed(1)
train = sample(1:nrow(clean_trips), nrow(clean_trips)*0.75)
trips.test=clean_trips[-train ,"flow"]
starts.test = clean_trips[-train ,"total.start"]

# best flow
set.seed(1)
rf.trips.best = randomForest(flow~weekdayend+District+Total.docks+bikelanedist+tstopdist+total.start+TempAvg+WindAvg+Precipitation,
                        data=clean_trips,
                        subset=train,
                        mtry=4,
                        ntree=100,
                        importance=TRUE)
set.seed(1)
yhat.rf.best = predict(rf.trips.best, newdata=clean_trips[-train,])
sprintf("Mean squared error on the test data for flow is %f",mean((yhat.rf.best-trips.test)^2)) #test MSE is 47, so on average off by ~11.5 (slightly better than initial model)
# plot(yhat.rf.2-trips.test) #no pattern to the residual, which is good
# varImpPlot(rf.trips.2, "Random Forest Importance -- All Activity Classes")

# best starts
set.seed(1)
rf.starts.best = randomForest(total.start~weekdayend+District+Total.docks+bikelanedist+tstopdist+TempAvg+WindAvg+Precipitation,
                         data=clean_trips,
                         subset=train,
                         mtry=8,
                         ntree=60)
yhat.starts = predict(rf.starts.best, newdata=clean_trips[-train,])
sprintf("Mean squared error on the test data for flow is %f", mean((yhat.starts-starts.test)^2)) #test MSE is 158, so on average off by 12 (better than SVM)
# importance(rf.starts.best)
# varImpPlot(rf.starts.best)
# summary(rf.starts.best)

rm(list=ls())
```

The best results for flow so far are superior to those found in the linear model, with MSE of 47 for flow and 158 for starts. We now want to see if we can get superior results by building a model for each activity level. We can also compare the most important predictors for the models created at each level to see if it varies by activity level. 

Cross-Validation was also performed for these models, but the charts are omitted here. Instead, we will present only the results for the best models for each activity class.

MSE for the high flow data is now much higher (768), while medium is similar but larger (62), and low (21) flow is much smaller. This makes sense given that "low" is the largest activity class, therefore lowering the test error overall when all classes are considered together. Overall, breaking modeling each activity class individually does not dramatically affect which predictors are most important. Though the exact order may change, the total number of docks and starts (measures of use) and T-stop and bike lane distances (measures of remoteness) are the most important factors. The District and whether it is a weekend jump around a bit in importance, while weather data has the smallest impact for all activity classes.

```{r figImportancePlots}
include_graphics(c("images/flow_high_importance.png","images/flow_medium_importance.png", "images/flow_low_importance.png"))
```

## Support Vector Machines

CLARA

## Local Regression

CLARA


# RQ3: Interpreting Results

PHIL AND CLARA




# Discussion and Conclusion

PHIL AND CLARA

Full project source code available at: https://github.com/cbelitz/blueBikes

# Final note about collaboration

-once the rest of the document is complete, a space for each of us to reflect on these questions:
How did your team work together? Did team members work together on pieces of the
project, or split the project into different topics for different team members?
